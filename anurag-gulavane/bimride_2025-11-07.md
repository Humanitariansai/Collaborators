
# Daily Progress Report – November 7, 2025

## Overview
Today I dedicated my work to understanding **distributed tracing**, a crucial observability technique that allows complex systems like Bimride to track the entire lifecycle of a request—from the moment a rider creates a trip request to the final moment when the trip is completed and payment is processed. Distributed tracing is the backbone of debugging, latency reduction, and system-wide performance visibility in modern microservice architectures.

Bimride will eventually consist of dozens of interconnected services: the ride service, matching engine, pricing engine, map service, location ingestion pipeline, driver communication service, notification service, payment processor, analytics service, and more. Each service will handle a part of a request before forwarding it to the next one. Without distributed tracing, understanding where delays happen would be nearly impossible.

Today, my focus was on implementing a conceptual tracing model for Bimride using OpenTelemetry principles, understanding span propagation, trace IDs, sampling strategies, latency budgeting, and how mobile clients send context downstream. I also explored real-world failure modes—like missing spans, broken context, or async jobs—and how proper tracing reduces debugging time by over 80%.

This was one of the most eye-opening topics so far, because it integrates everything: networking, backend architecture, logs, monitoring, client behavior, and the entire flow of the system. With tracing, Bimride becomes observable and predictable, even under high load.

---

## Activities

### 1. Studied the basics of tracing architecture
Distributed tracing consists of:
- **Trace** → the entire journey of a request  
- **Spans** → individual operations inside a trace  
- **Span context** → metadata that allows correlation  
- **Parent-child relationships** → show causal structure  

I visualized an example of a Bimride trace:
- Rider opens app → Fetch nearby drivers (Frontend Span)  
- API Gateway (Span)  
- Location Service (Span)  
- Matching Service (Span)  
- Pricing Service (Span)  
- Notification Service (Span)  

Each of these spans reveals execution time, metadata, logs, and dependencies.

### 2. Learned how trace IDs propagate across microservices
All services must pass the same trace ID across network boundaries.

Propagation mechanisms include:
- HTTP headers (w3c `traceparent`)  
- gRPC metadata  
- messaging headers (Kafka/Pulsar)  

For Bimride:
- Mobile app creates root trace for interactions  
- Backend attaches context to downstream service calls  
- Async jobs must reattach parent context  

This ensures all spans belong to the same trace.

### 3. Designed a tracing model for trip request flow
I documented the following spans:

1. **App Initiated Request (Frontend Span)**  
2. **API Gateway Span**  
3. **Authentication Check Span**  
4. **Ride Service Span**  
5. **Matching Engine Span**  
6. **Location Query Span**  
7. **Driver ETA Calculation Span**  
8. **Pricing Pre-Estimate Span**  
9. **Notification Dispatch Span**  

A complete trace would allow Bimride engineers to instantly see:
- where processing slowed  
- which service caused a bottleneck  
- whether database or network latency spiked  
- how long each step took  

This is essential in tourism-heavy peak times.

### 4. Explored sampling strategies for cost control
Full tracing is expensive.

Options:
- **Always On** → expensive but detailed  
- **Probability Sampling** → trace 5% or 10% of requests  
- **Head Sampling** → decision made at trace start  
- **Tail Sampling** → decision made after seeing slow requests  

For Bimride:
- 100% sampling for failed or slow requests  
- 10% sampling for normal requests  
- Lower sampling for background tasks  

This balances cost and insight.

### 5. Studied baggage vs span attributes
Baggage is metadata that travels with the request, such as:
- rider ID  
- driver ID  
- trip ID  
- region ID  

Span attributes are metadata for specific operations.

Understanding when to use each prevents performance issues.

### 6. Implemented theoretical latency budgets
A latency budget assigns how much time each service is allowed to take.

Example for Bimride’s driver matching:
- Gateway → 20ms  
- Auth → 10ms  
- Ride service → 30ms  
- Matching → 80ms  
- Pricing → 25ms  

Total budget: 165ms  
This ensures the rider experience remains snappy.

Tracing allows automatic budget enforcement.

### 7. Explored async operations and trace continuation
Matching engines often use async workers.

Problems:
- lost context  
- missing spans  
- fragmented traces  

Solution:
- explicitly pass parent context into the worker  
- store trace IDs in job metadata  

This ensures full visibility across asynchronous flows.

### 8. Designed error tracing for Bimride
Error spans capture:
- stack trace  
- error code  
- request context  
- replay path  

For example:
If pricing fails during surge calculation, tracing reveals:
- exact input values  
- which microservice failed  
- how far the request progressed  

This reduces debugging time dramatically.

### 9. Built mental visualizations of trace timelines
Trace UIs (like Jaeger or Zipkin) show waterfall charts:
- wide bars → slow operations  
- gaps → network delays  
- parallel spans → concurrent calls  

Visual patterns reveal:
- slow database queries  
- failing services  
- high network RTT  
- redundant operations  

Bimride’s engineering team would rely on these daily.

---

## How Today’s Work Ties to Bimride

### 1. Faster debugging and shorter outage durations
Without tracing:
- problems are invisible  
- guesswork dominates  
- each fix takes hours  

With tracing:
- outages shrink from 1 hour → 5–10 minutes  
- root cause is immediately visible  
- engineers know exactly which service misbehaved  

This leads to higher uptime and user trust.

### 2. Better performance under high load
Tracing reveals performance bottlenecks like:
- slow DB queries  
- slow map service calls  
- slow pricing recalculations  

Bimride can optimize these quickly, ensuring smooth operation even when thousands of tourists use the app during peak seasons.

### 3. Allows intelligent service-level tuning
Tracing enables:
- dynamic scaling  
- rate limiting  
- circuit breaking  
- adaptive routing  

These ensure stability during stress.

### 4. Essential for customer support investigations
When a rider claims:
- “The driver never moved,”  
- “The app didn’t assign me fast enough,”  
- “The fare changed unexpectedly,”  

Support agents can view a full trace to see exactly what happened.

### 5. Improves driver experience
Drivers depend on fast matching and responsive app behavior.  
Tracing identifies latency affecting their income.

### 6. Forms the backbone of future monitoring
Distributed tracing integrates with:
- logs  
- metrics  
- dashboards  
- anomaly detection  

It becomes Bimride’s observability core.

---

## Extended Weekly Reflection
This week, I realized how interconnected Bimride’s architecture truly is. Every topic—ride lifecycle modeling, session security, event sourcing, sensor fusion, and distributed tracing—revealed that mobility systems function like living organisms. Each subsystem affects others, and a single flaw can ripple through the entire platform.

Three lessons stood out:

### **1. Correctness + Observability = Reliability**
A beautifully designed system is meaningless if you cannot observe it. Distributed tracing made me see that reliability is not passive; it’s engineered through visibility.

### **2. Barbados’ real-world constraints matter**
Network instability, road layouts, tourism patterns—all influence:
- session refresh  
- lifecycle transitions  
- pricing logic  
- GPS fusion reliability  
- tracing coverage  

Bimride must be engineered *for* the Caribbean environment, not in spite of it.

### **3. Good architecture compounds**
Everything I learned this week will pay dividends later:
- event sourcing → trip integrity  
- sensor fusion → map accuracy  
- tracing → performance visibility  
- lifecycle modeling → reliability  
- session security → user trust  

These systems reinforce each other like gears in a machine.

I feel much more connected to the long-term technical vision of Bimride—building something not just functional, but *strong, resilient, and precise*.

